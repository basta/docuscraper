--- Content from https://pettingzoo.farama.org/ ---

Back to top Edit this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar An API standard for multi-agent reinforcement learning. PettingZoo is a simple, pythonic interface capable of representing general multi-agent reinforcement learning (MARL) problems. PettingZoo includes a wide variety of reference environments, helpful utilities, and tools for creating your own custom environments. The AEC API supports sequential turn based environments, while the Parallel API supports environments with simultaneous actions. Environments can be interacted with using a similar interface to Gymnasium : from pettingzoo.butterfly import knights_archers_zombies_v10 env = knights_archers_zombies_v10 . env ( render_mode = "human" ) env . reset ( seed = 42 ) for agent in env . agent_iter (): observation , reward , termination , truncation , info = env . last () if termination or truncation : action = None else : # this is where you would insert your policy action = env . action_space ( agent ) . sample () env . step ( action )

--- Content from https://pettingzoo.farama.org/api/aec/ ---

Back to top Edit this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar AEC API ¶ By default, PettingZoo models games as Agent Environment Cycle (AEC) environments. This allows PettingZoo to represent any type of game multi-agent RL can consider. For more information, see About AEC or PettingZoo: A Standard API for Multi-Agent Reinforcement Learning . PettingZoo Wrappers can be used to convert between Parallel and AEC environments, with some restrictions (e.g., an AEC env must only update once at the end of each cycle). Examples ¶ PettingZoo Classic provides standard examples of AEC environments for turn-based games, many of which implement Illegal Action Masking . We provide a tutorial for creating a simple Rock-Paper-Scissors AEC environment, showing how games with simultaneous actions can also be represented with AEC environments. Usage ¶ AEC environments can be interacted with as follows: from pettingzoo.classic import rps_v2 env = rps_v2 . env ( render_mode = "human" ) env . reset ( seed = 42 ) for agent in env . agent_iter (): observation , reward , termination , truncation , info = env . last () if termination or truncation : action = None else : action = env . action_space ( agent ) . sample () # this is where you would insert your policy env . step ( action ) env . close () Action Masking ¶ AEC environments often include action masks, in order to mark valid/invalid actions for the agent. To sample actions using action masking: from pettingzoo.classic import chess_v6 env = chess_v6 . env ( render_mode = "human" ) env . reset ( seed = 42 ) for agent in env . agent_iter (): observation , reward , termination , truncation , info = env . last () if termination or truncation : action = None else : # invalid action masking is optional and environment-dependent if "action_mask" in info : mask = info [ "action_mask" ] elif isinstance ( observation , dict ) and "action_mask" in observation : mask = observation [ "action_mask" ] else : mask = None action = env . action_space ( agent ) . sample ( mask ) # this is where you would insert your policy env . step ( action ) env . close () Note: action masking is optional, and can be implemented using either observation or info . PettingZoo Classic environments store action masks in the observation dict: mask = observation["action_mask"] Shimmy ’s OpenSpiel environments stores action masks in the info dict: mask = info["action_mask"] To implement action masking in a custom environment, see Custom Environment: Action Masking For more information on action masking, see A Closer Look at Invalid Action Masking in Policy Gradient Algorithms (Huang, 2022) About AEC ¶ The Agent Environment Cycle (AEC) model was designed as a Gym -like API for MARL, supporting all possible use cases and types of environments. This includes environments with: Large number of agents (see Magent2 ) Variable number of agents (see Knights, Archers, Zombies ) Action and observation spaces of any type (e.g., Box , Discrete , MultiDiscrete , MultiBinary , Text ) Nested action and observation spaces (e.g., Dict , Tuple , Sequence , Graph ) Support for action masking (see Classic environments) Action and observation spaces which can change over time, and differ per agent (see generated_agents and variable_env_test ) Changing turn order and evolving environment dynamics (e.g., games with multiple stages, reversing turns) In an AEC environment, agents act sequentially, receiving updated observations and rewards before taking an action. The environment updates after each agent’s step, making it a natural way of representing sequential games such as Chess. The AEC model is flexible enough to handle any type of game that multi-agent RL can consider. This is in contrast to the Partially Observable Stochastic Game (POSG) model, represented in our Parallel API , where agents act simultaneously and can only receive observations and rewards at the end of a cycle.

This makes it difficult to represent sequential games, and results in race conditions–where agents choose to take actions which are mutually exclusive. This causes environment behavior to differ depending on internal resolution of agent order, resulting in hard-to-detect bugs if even a single race condition is not caught and handled by the environment (e.g., through tie-breaking). The AEC model is similar to Extensive Form Games (EFGs) model, used in DeepMind’s OpenSpiel .

EFGs represent sequential games as trees, explicitly representing every possible sequence of actions as a root to leaf path in the tree.

A limitation of EFGs is that the formal definition is specific to game-theory, and only allows rewards at the end of a game, whereas in RL, learning often requires frequent rewards. EFGs can be extended to represent stochastic games by adding a player representing the environment (e.g., chance nodes in OpenSpiel), which takes actions according to a given probability distribution. However, this requires users to manually sample and apply chance node actions whenever interacting with the environment, leaving room for user error and potential random seeding issues. AEC environments, in contrast, handle environment dynamics internally after each agent step, resulting in a simpler mental model of the environment, and allowing for arbitrary and evolving environment dynamics (as opposed to static chance distribution). The AEC model also more closely resembles how computer games are implemented in code, and can be thought of similar to the game loop in game programming. For more information about the AEC model and PettingZoo’s design philosophy, see PettingZoo: A Standard API for Multi-Agent

Reinforcement Learning . AECEnv ¶ class pettingzoo.utils.env. AECEnv [source] ¶ The AECEnv steps agents one at a time. If you are unsure if you have implemented a AECEnv correctly, try running

the api_test documented in the Developer documentation on the website. Attributes ¶ AECEnv. agents : list [ AgentID ] ¶ A list of the names of all current agents, typically integers. These may be changed as an environment progresses (i.e. agents can be added or removed). Type : List[AgentID] AECEnv. num_agents ¶ The length of the agents list. AECEnv. possible_agents : list [ AgentID ] ¶ A list of all possible_agents the environment could generate. Equivalent to the list of agents in the observation and action spaces. This cannot be changed through play or resetting. Type : List[AgentID] AECEnv. max_num_agents ¶ The length of the possible_agents list. AECEnv. agent_selection : AgentID ¶ An attribute of the environment corresponding to the currently selected agent that an action can be taken for. Type : AgentID AECEnv. terminations : dict [ AgentID , bool ] ¶ AECEnv. truncations : dict [ AgentID , bool ] ¶ AECEnv. rewards : dict [ AgentID , float ] ¶ A dict of the rewards of every current agent at the time called, keyed by name. Rewards the instantaneous reward generated after the last step. Note that agents can be added or removed from this attribute. last() does not directly access this attribute, rather the returned reward is stored in an internal variable. The rewards structure looks like: { 0 :[ first agent reward ], 1 :[ second agent reward ] ... n - 1 :[ nth agent reward ]} Type : Dict[AgentID, float] AECEnv. infos : dict [ AgentID , dict [ str , Any ] ] ¶ A dict of info for each current agent, keyed by name. Each agent’s info is also a dict. Note that agents can be added or removed from this attribute. last() accesses this attribute. The returned dict looks like: infos = { 0 :[ first agent info ], 1 :[ second agent info ] ... n - 1 :[ nth agent info ]} Type : Dict[AgentID, Dict[str, Any]] AECEnv. observation_spaces : dict [ AgentID , Space ] ¶ A dict of the observation spaces of every agent, keyed by name. This cannot be changed through play or resetting. Type : Dict[AgentID, gymnasium.spaces.Space] AECEnv. action_spaces : dict [ AgentID , Space ] ¶ A dict of the action spaces of every agent, keyed by name. This cannot be changed through play or resetting. Type : Dict[AgentID, gymnasium.spaces.Space] Methods ¶ AECEnv. step ( action : ActionType ) → None [source] ¶ Accepts and executes the action of the current agent_selection in the environment. Automatically switches control to the next agent. AECEnv. reset ( seed : int | None = None , options : dict | None = None ) → None [source] ¶ Resets the environment to a starting state. AECEnv. observe ( agent : AgentID ) → ObsType | None [source] ¶ Returns the observation an agent currently can make. last() calls this function. AECEnv. render ( ) → None | np.ndarray | str | list [source] ¶ Renders the environment as specified by self.render_mode. Render mode can be human to display a window.

Other render modes in the default environments are ‘rgb_array’ which returns a numpy array and is supported by all environments outside of classic,

and ‘ansi’ which returns the strings printed (specific to classic environments). AECEnv. close ( ) [source] ¶ Closes any resources that should be released. Closes the rendering window, subprocesses, network connections,

or any other resources that should be released.

--- Content from https://pettingzoo.farama.org/api/parallel/ ---

Back to top Edit this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Parallel API ¶ In addition to the main API, we have a secondary parallel API for environments where all agents have simultaneous actions and observations. An environment with parallel API support can be created via <game>.parallel_env() . This API is based around the paradigm of Partially Observable Stochastic Games (POSGs) and the details are similar to RLlib’s MultiAgent environment specification , except we allow for different observation and action spaces between the agents. For a comparison with the AEC API, see About AEC . For more information, see PettingZoo: A Standard API for Multi-Agent Reinforcement Learning . PettingZoo Wrappers can be used to convert between Parallel and AEC environments, with some restrictions (e.g., an AEC env must only update once at the end of each cycle). Examples ¶ PettingZoo Butterfly provides standard examples of Parallel environments, such as Pistonball . We provide tutorials for creating two custom Parallel environments: Rock-Paper-Scissors (Parallel) , and a simple gridworld environment Usage ¶ Parallel environments can be interacted with as follows: from pettingzoo.butterfly import pistonball_v6 parallel_env = pistonball_v6 . parallel_env ( render_mode = "human" ) observations , infos = parallel_env . reset ( seed = 42 ) while parallel_env . agents : # this is where you would insert your policy actions = { agent : parallel_env . action_space ( agent ) . sample () for agent in parallel_env . agents } observations , rewards , terminations , truncations , infos = parallel_env . step ( actions ) parallel_env . close () ParallelEnv ¶ class pettingzoo.utils.env. ParallelEnv [source] ¶ Parallel environment class. It steps every live agent at once. If you are unsure if you

have implemented a ParallelEnv correctly, try running the parallel_api_test in

the Developer documentation on the website. agents ¶ A list of the names of all current agents, typically integers. These may be changed as an environment progresses (i.e. agents can be added or removed). Type : list[AgentID] num_agents ¶ The length of the agents list. Type : int possible_agents ¶ A list of all possible_agents the environment could generate. Equivalent to the list of agents in the observation and action spaces. This cannot be changed through play or resetting. Type : list[AgentID] max_num_agents ¶ The length of the possible_agents list. Type : int observation_spaces ¶ A dict of the observation spaces of every agent, keyed by name. This cannot be changed through play or resetting. Type : Dict[AgentID, gym.spaces.Space] action_spaces ¶ A dict of the action spaces of every agent, keyed by name. This cannot be changed through play or resetting. Type : Dict[AgentID, gym.spaces.Space] step ( actions : dict [ AgentID , ActionType ] ) → tuple [ dict [ AgentID , ObsType ] , dict [ AgentID , float ] , dict [ AgentID , bool ] , dict [ AgentID , bool ] , dict [ AgentID , dict ] ] [source] ¶ Receives a dictionary of actions keyed by the agent name. Returns the observation dictionary, reward dictionary, terminated dictionary, truncated dictionary

and info dictionary, where each dictionary is keyed by the agent. reset ( seed : int | None = None , options : dict | None = None ) → tuple [ dict [ AgentID , ObsType ] , dict [ AgentID , dict ] ] [source] ¶ Resets the environment. And returns a dictionary of observations (keyed by the agent name) render ( ) → None | np.ndarray | str | list [source] ¶ Displays a rendered frame from the environment, if supported. Alternate render modes in the default environments are ‘rgb_array’ which returns a numpy array and is supported by all environments outside

of classic, and ‘ansi’ which returns the strings printed

(specific to classic environments). close ( ) [source] ¶ Closes the rendering window. state ( ) → ndarray [source] ¶ Returns the state. State returns a global view of the environment appropriate for

centralized training decentralized execution methods like QMIX observation_space ( agent : AgentID ) → Space [source] ¶ Takes in agent and returns the observation space for that agent. MUST return the same value for the same agent name Default implementation is to return the observation_spaces dict action_space ( agent : AgentID ) → Space [source] ¶ Takes in agent and returns the action space for that agent. MUST return the same value for the same agent name Default implementation is to return the action_spaces dict

--- Content from https://pettingzoo.farama.org/api/utils/ ---

Back to top Edit this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Utils ¶ PettingZoo has an assortment of helper utilities which provide additional functionality for interacting with environments. Note: see also PettingZoo Wrappers , which provide additional functionality for customizing environments. Average Total Reward ¶ pettingzoo.utils.average_total_reward. average_total_reward ( env : AECEnv , max_episodes : int = 100 , max_steps : int = 10000000000 ) → float [source] ¶ Calculates the average total reward over the episodes for AEC environments. Runs an env object with random actions until either max_episodes or

max_steps is reached.

Reward is summed across all agents, making it unsuited for use in zero-sum

games. The average total reward for an environment, as presented in the documentation, is summed over all agents over all steps in the episode, averaged over episodes. This value is important for establishing the simplest possible baseline: the random policy. from pettingzoo.utils import average_total_reward from pettingzoo.butterfly import pistonball_v6 env = pistonball_v6 . env () average_total_reward ( env , max_episodes = 100 , max_steps = 10000000000 ) Where max_episodes and max_steps both limit the total number of evaluations (when the first is hit evaluation stops) Observation Saving ¶ pettingzoo.utils.save_observation. save_observation ( env : AECEnv [ AgentID , Any , Any ] , agent : AgentID | None = None , all_agents : bool = False , save_dir : str = os.getcwd() ) → None [source] ¶ If the agents in a game make observations that are images then the observations can be saved to an image file. This function takes in the environment, along with a specified agent. If no agent is specified, then the current selected agent for the environment is chosen. If all_agents is passed in as True , then the observations of all agents in the environment is saved. By default, the images are saved to the current working directory in a folder matching the environment name. The saved image will match the name of the observing agent. If save_dir is passed in, a new folder is created where images will be saved to. This function can be called during training/evaluation if desired, which is why environments have to be reset before it can be used. from pettingzoo.utils import save_observation from pettingzoo.butterfly import pistonball_v6 env = pistonball_v6 . env () env . reset ( seed = 42 ) save_observation ( env , agent = None , all_agents = False ) Capture Stdout ¶ Base class which is used by CaptureStdoutWrapper . Captures system standard out as a string value in a variable. class pettingzoo.utils.capture_stdout. capture_stdout [source] ¶ Class allowing to capture stdout. Example >>> from pettingzoo.utils.capture_stdout import capture_stdout >>> with capture_stdout () as var : ... print ( "test" ) ... data = var . getvalue () ... >>> data 'test\n' Agent Selector ¶ The agent selector utility allows for easy cycling of agents in an AEC environment. At any time it can be reset or reinitialized with a new order, allowing for changes in turn order or handling a dynamic number of agents (see Knights-Archers-Zombies for an example of spawning/killing agents) Note: while many PettingZoo environments use AgentSelector to manage agent cycling internally, it is not intended to be used externally when interacting with an environment. Instead, use for agent in env.agent_iter() (see AEC API Usage ). class pettingzoo.utils.agent_selector. AgentSelector ( agent_order : list [ Any ] ) [source] ¶ Outputs an agent in the given order whenever agent_select is called. Can reinitialize to a new order. Example >>> from pettingzoo.utils import AgentSelector >>> agent_selector = AgentSelector ( agent_order = [ "player1" , "player2" ]) >>> agent_selector . reset () 'player1' >>> agent_selector . next () 'player2' >>> agent_selector . is_last () True >>> agent_selector . reinit ( agent_order = [ "player2" , "player1" ]) >>> agent_selector . next () 'player2' >>> agent_selector . is_last () False is_first ( ) → bool [source] ¶ Check if the current agent is the first agent in the cycle. is_last ( ) → bool [source] ¶ Check if the current agent is the last agent in the cycle. next ( ) → Any [source] ¶ Get the next agent. reinit ( agent_order : list [ Any ] ) → None [source] ¶ Reinitialize to a new order. reset ( ) → Any [source] ¶ Reset to the original order. class pettingzoo.utils.agent_selector. agent_selector ( * args , ** kwargs ) [source] ¶ Deprecated version of AgentSelector. Use that instead. EnvLogger ¶ EnvLogger provides functionality for common warnings and errors for environments, and allows for custom messages. It is used internally in PettingZoo Wrappers . class pettingzoo.utils.env_logger. EnvLogger [source] ¶ Used for logging warnings and errors for environments. static error_agent_iter_before_reset ( ) → None [source] ¶ Error: reset() needs to be called before agent_iter(). . static error_nan_action ( ) → None [source] ¶ Error: step() cannot take in a nan action. . static error_observe_before_reset ( ) → None [source] ¶ Error: reset() needs to be called before observe. . static error_possible_agents_attribute_missing ( name : str ) → None [source] ¶ Warns: [ERROR]: This environment does not support {attribute}. . static error_render_before_reset ( ) → None [source] ¶ Error: reset() needs to be called before render. . static error_state_before_reset ( ) → None [source] ¶ Error: reset() needs to be called before state. . static error_step_before_reset ( ) → None [source] ¶ Error: reset() needs to be called before step. . static flush ( ) → None [source] ¶ Flushes EnvLogger output. static get_logger ( ) → Logger [source] ¶ Returns the logger object. mqueue : list [ Any ] = [] ¶ static suppress_output ( ) → None [source] ¶ Suppresses EnvLogger output. static unsuppress_output ( ) → None [source] ¶ Resets EnvLogger output to be printed. static warn_action_out_of_bound ( action : Any , action_space : Space , backup_policy : str ) → None [source] ¶ Warns: [WARNING]: Received an action {action} that was outside action space {action_space}. . static warn_on_illegal_move ( ) → None [source] ¶ Warns: [WARNING]: Illegal move made, game terminating with current player losing. . static warn_step_after_terminated_truncated ( ) → None [source] ¶ Warns: [WARNING]: step() called after all agents are terminated or truncated. Should reset() first. .

--- Content from https://pettingzoo.farama.org/api/wrappers/ ---

Back to top Edit this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Wrappers ¶ Using Wrappers ¶ A wrapper is an environment transformation that takes in an environment as input, and outputs a new environment that is similar to the input environment, but with some transformation or validation applied. The following wrappers can be used with PettingZoo environments: PettingZoo Wrappers include conversion wrappers to convert between the AEC and Parallel APIs, and a set of simple utility wrappers which provide input validation and other convenient reusable logic. Supersuit Wrappers include commonly used pre-processing functions such as frame-stacking and color reduction, compatible with both PettingZoo and Gymnasium. Shimmy Compatibility Wrappers allow commonly used external reinforcement learning environments to be used with PettingZoo and Gymnasium.

--- Content from https://pettingzoo.farama.org/api/wrappers/pz_wrappers/ ---

Back to top Edit this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar PettingZoo Wrappers ¶ PettingZoo includes the following types of wrappers: Conversion Wrappers : wrappers for converting environments between the AEC and Parallel APIs Utility Wrappers : a set of wrappers which provide convenient reusable logic, such as enforcing turn order or clipping out-of-bounds actions. Conversion wrappers ¶ AEC to Parallel ¶ pettingzoo.utils.conversions. aec_to_parallel ( aec_env : AECEnv [ AgentID , ObsType , ActionType ] ) → ParallelEnv [ AgentID , ObsType , ActionType ] [source] ¶ Converts an AEC environment to a Parallel environment. In the case of an existing Parallel environment wrapped using a parallel_to_aec_wrapper , this function will return the original Parallel environment.

Otherwise, it will apply the aec_to_parallel_wrapper to convert the environment. An environment can be converted from an AEC environment to a parallel environment with the aec_to_parallel wrapper shown below. Note that this wrapper makes the following assumptions about the underlying environment: The environment steps in a cycle, i.e. it steps through every live agent in order. The environment does not update the observations of the agents except at the end of a cycle. Most parallel environments in PettingZoo only allocate rewards at the end of a cycle. In these environments, the reward scheme of the AEC API an the parallel API is equivalent. If an AEC environment does allocate rewards within a cycle, then the rewards will be allocated at different timesteps in the AEC environment an the Parallel environment. In particular, the AEC environment will allocate all rewards from one time the agent steps to the next time, while the Parallel environment will allocate all rewards from when the first agent stepped to the last agent stepped. To convert an AEC environment into a parallel environment: from pettingzoo.utils.conversions import aec_to_parallel from pettingzoo.butterfly import pistonball_v6 env = pistonball_v6 . env () env = aec_to_parallel ( env ) Parallel to AEC ¶ pettingzoo.utils.conversions. parallel_to_aec ( par_env : ParallelEnv [ AgentID , ObsType , ActionType | None ] ) → AECEnv [ AgentID , ObsType , ActionType | None ] [source] ¶ Converts a Parallel environment to an AEC environment. In the case of an existing AEC environment wrapped using a aec_to_parallel_wrapper , this function will return the original AEC environment.

Otherwise, it will apply the parallel_to_aec_wrapper to convert the environment. Any parallel environment can be efficiently converted to an AEC environment with the parallel_to_aec wrapper. To convert a parallel environment into an AEC environment: from pettingzoo.utils import parallel_to_aec from pettingzoo.butterfly import pistonball_v6 env = pistonball_v6 . parallel_env () env = parallel_to_aec ( env ) Utility Wrappers ¶ We wanted our pettingzoo environments to be both easy to use and easy to implement. To combine these, we have a set of simple wrappers which provide input validation and other convenient reusable logic. You can apply these wrappers to your environment in a similar manner to the below examples: To wrap an AEC environment: from pettingzoo.utils import TerminateIllegalWrapper from pettingzoo.classic import tictactoe_v3 env = tictactoe_v3 . env () env = TerminateIllegalWrapper ( env , illegal_reward =- 1 ) env . reset () for agent in env . agent_iter (): observation , reward , termination , truncation , info = env . last () if termination or truncation : action = None else : action = env . action_space ( agent ) . sample () # this is where you would insert your policy env . step ( action ) env . close () Note: Most AEC environments include TerminateIllegalWrapper in their initialization, so this code does not change the environment’s behavior. To wrap a Parallel environment. from pettingzoo.utils import BaseParallelWrapper from pettingzoo.butterfly import pistonball_v6 parallel_env = pistonball_v6 . parallel_env ( render_mode = "human" ) parallel_env = BaseParallelWrapper ( parallel_env ) observations , infos = parallel_env . reset () while parallel_env . agents : actions = { agent : parallel_env . action_space ( agent ) . sample () for agent in parallel_env . agents } # this is where you would insert your policy observations , rewards , terminations , truncations , infos = parallel_env . step ( actions ) Warning Included PettingZoo wrappers currently do not support parallel environments, to use them you must convert your environment to AEC, apply the wrapper, and convert back to parallel. from pettingzoo.utils import ClipOutOfBoundsWrapper from pettingzoo.sisl import multiwalker_v9 from pettingzoo.utils import aec_to_parallel parallel_env = multiwalker_v9 . env ( render_mode = "human" ) parallel_env = ClipOutOfBoundsWrapper ( parallel_env ) parallel_env = aec_to_parallel ( parallel_env ) observations , infos = parallel_env . reset () while parallel_env . agents : actions = { agent : parallel_env . action_space ( agent ) . sample () for agent in parallel_env . agents } # this is where you would insert your policy observations , rewards , terminations , truncations , infos = parallel_env . step ( actions ) class pettingzoo.utils.wrappers. BaseWrapper ( env : AECEnv [ AgentID , ObsType , ActionType ] ) [source] ¶ Creates a wrapper around env parameter. All AECEnv wrappers should inherit from this base class class pettingzoo.utils.wrappers. TerminateIllegalWrapper ( env : AECEnv [ AgentID , ObsType , ActionType ] , illegal_reward : float ) [source] ¶ This wrapper terminates the game with the current player losing in case of illegal values. Parameters : illegal_reward – number that is the value of the player making an illegal move. class pettingzoo.utils.wrappers. CaptureStdoutWrapper ( env : AECEnv ) [source] ¶ Takes an environment which prints to terminal, and gives it an ansi render mode where it captures the terminal output and returns it as a string instead. class pettingzoo.utils.wrappers. AssertOutOfBoundsWrapper ( env : AECEnv [ AgentID , ObsType , ActionType ] ) [source] ¶ Asserts if the action given to step is outside of the action space. class pettingzoo.utils.wrappers. ClipOutOfBoundsWrapper ( env : AECEnv ) [source] ¶ Clips the input action to fit in the continuous action space (emitting a warning if it does so). Applied to continuous environments in pettingzoo. class pettingzoo.utils.wrappers. OrderEnforcingWrapper ( env : AECEnv [ AgentID , ObsType , ActionType ] ) [source] ¶ Checks if function calls or attribute access are in a disallowed order. The following are raised:

* AttributeError if any of the following are accessed before reset(): rewards, terminations, truncations, infos, agent_selection,

num_agents, agents. An error if any of the following are called before reset:

render(), step(), observe(), state(), agent_iter() A warning if step() is called when there are no agents remaining.

--- Content from https://pettingzoo.farama.org/api/wrappers/shimmy_wrappers/ ---

Back to top Edit this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Shimmy Compatibility Wrappers ¶ The Shimmy package ( pip install shimmy ) allows commonly used external reinforcement learning environments to be used with PettingZoo and Gymnasium. Supported multi-agent environments: ¶ OpenSpiel ¶ 70+ implementations of various board games DeepMind Control Soccer ¶ Multi-agent robotics environment where teams of agents compete in soccer. DeepMind Melting Pot ¶ Suite of test scenarios for multi-agent reinforcement learning Assesses generalization to novel social situations: familiar and unfamiliar individuals social interactions: cooperation, competition, deception, reciprocation, trust, stubbornness 50+ substrates and 250+ test scenarios Usage ¶ To load a DeepMind Control multi-agent soccer game : from shimmy import DmControlMultiAgentCompatibilityV0 from dm_control.locomotion import soccer as dm_soccer env = dm_soccer . load ( team_size = 2 ) env = DmControlMultiAgentCompatibilityV0 ( env , render_mode = "human" ) observations , infos = env . reset () while env . agents : actions = { agent : env . action_space ( agent ) . sample () for agent in env . agents } # this is where you would insert your policy observations , rewards , terminations , truncations , infos = env . step ( actions ) To load an OpenSpiel game of backgammon , wrapped with TerminateIllegalWrapper : from shimmy import OpenSpielCompatibilityV0 from pettingzoo.utils import TerminateIllegalWrapper env = OpenSpielCompatibilityV0 ( game_name = "chess" , render_mode = None ) env = TerminateIllegalWrapper ( env , illegal_reward =- 1 ) env . reset () for agent in env . agent_iter (): observation , reward , termination , truncation , info = env . last () if termination or truncation : action = None else : action = env . action_space ( agent ) . sample ( info [ "action_mask" ]) # this is where you would insert your policy env . step ( action ) env . render () To load a Melting Pot prisoner’s dilemma in the matrix substrate: from shimmy import MeltingPotCompatibilityV0 env = MeltingPotCompatibilityV0 ( substrate_name = "prisoners_dilemma_in_the_matrix__arena" , render_mode = "human" ) observations , infos = env . reset () while env . agents : actions = { agent : env . action_space ( agent ) . sample () for agent in env . agents } observations , rewards , terminations , truncations , infos = env . step ( actions ) env . step ( actions ) env . close () For more information, see Shimmy documentation . Multi-Agent Compatibility Wrappers: ¶ shimmy.dm_control_multiagent_compatibility.DmControlMultiAgentCompatibilityV0 shimmy.openspiel_compatibility.OpenSpielCompatibilityV0 shimmy.meltingpot_compatibility.MeltingPotCompatibilityV0 Citation ¶ If you use this in your research, please cite: @software { shimmy2022github , author = {{ Jun Jet Tai , Mark Towers , Elliot Tower } and Jordan Terry }, title = { Shimmy : Gymnasium and PettingZoo Wrappers for Commonly Used Environments }, url = { https : // github . com / Farama - Foundation / Shimmy }, version = { 1.0.0 }, year = { 2022 }, }

--- Content from https://pettingzoo.farama.org/api/wrappers/supersuit_wrappers/ ---

Back to top Edit this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Supersuit Wrappers ¶ The SuperSuit companion package ( pip install supersuit ) includes a collection of pre-processing functions which can applied to both AEC and Parallel environments. To convert space invaders to a greyscale observation space and stack the last 4 frames: from pettingzoo.atari import space_invaders_v2 from supersuit import color_reduction_v0 , frame_stack_v1 env = space_invaders_v2 . env () env = frame_stack_v1 ( color_reduction_v0 ( env , 'full' ), 4 ) Included Functions ¶ Supersuit includes the following wrappers: clip_reward_v0 ( env , lower_bound = -1 , upper_bound = 1 ) ¶ Clips rewards to between lower_bound and upper_bound. This is a popular way of handling rewards with significant variance of magnitude, especially in Atari environments. clip_actions_v0 ( env ) ¶ Clips Box actions to be within the high and low bounds of the action space. This is a standard transformation applied to environments with continuous action spaces to keep the action passed to the environment within the specified bounds. color_reduction_v0 ( env , mode = 'full' ) ¶ Simplifies color information in graphical ((x,y,3) shaped) environments. mode=’full’ fully greyscales of the observation. This can be computationally intensive. Arguments of ‘R’, ‘G’ or ‘B’ just take the corresponding R, G or B color channel from observation. This is much faster and is generally sufficient. dtype_v0 ( env , dtype ) ¶ Recasts your observation as a certain dtype. Many graphical games return uint8 observations, while neural networks generally want float16 or float32 . dtype can be anything NumPy would except as a dtype argument (e.g. np.dtype classes or strings). flatten_v0 ( env ) ¶ flattens observations into a 1D array. frame_skip_v0 ( env , num_frames ) ¶ Skips num_frames number of frames by reapplying old actions over and over. Observations skipped over are ignored. Rewards skipped over are accumulated. Like Gymnasium Atari’s frameskip parameter, num_frames can also be a tuple (min_skip, max_skip) , which indicates a range of possible skip lengths which are randomly chosen from (in single agent environments only). delay_observations_v0 ( env , delay ) ¶ Delays observation by delay frames. Before delay frames have been executed, the observation is all zeros. Along with frame_skip, this is the preferred way to implement reaction time for high FPS games. sticky_actions_v0 ( env , repeat_action_probability ) ¶ Assigns a probability of an old action “sticking” to the environment and not updating as requested. This is to prevent agents from learning predefined action patterns in highly deterministic games like Atari. Note that the stickiness is cumulative, so an action has a repeat_action_probability^2 chance of an action sticking for two turns in a row, etc. This is the recommended way of adding randomness to Atari by “Machado et al. (2018), “Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents” frame_stack_v1 ( env , num_frames = 4 ) ¶ Stacks the most recent frames. For vector games observed via plain vectors (1D arrays), the output is just concatenated to a longer 1D array. 2D or 3D arrays are stacked to be taller 3D arrays. At the start of the game, frames that don’t yet exist are filled with 0s. num_frames=1 is analogous to not using this function. max_observation_v0 ( env , memory ) ¶ The resulting observation becomes the max over memory number of prior frames. This is important for Atari environments, as many games have elements that are intermitently flashed on the instead of being constant, due to the peculiarities of the console and CRT TVs. The OpenAI baselines MaxAndSkip Atari wrapper is equivalent to doing memory=2 and then a frame_skip of 4. normalize_obs_v0 ( env , env_min = 0 , env_max = 1 ) ¶ Linearly scales observations to the range env_min (default 0) to env_max (default 1), given the known minimum and maximum observation values defined in the observation space. Only works on Box observations with float32 or float64 dtypes and finite bounds. If you wish to normalize another type, you can first apply the dtype wrapper to convert your type to float32 or float64. reshape_v0 ( env , shape ) ¶ Reshapes observations into given shape. resize_v1 ( env , x_size , y_size , linear_interp = False ) ¶ Performs interpolation to up-size or down-size observation image using area interpolation by default. Linear interpolation is also available by setting linear_interp=True (it’s faster and better for up-sizing). This wrapper is only available for 2D or 3D observations, and only makes sense if the observation is an image. nan_noop_v0 ( env ) ¶ If an action is a NaN value for a step, the following wrapper will trigger a warning and perform a no operation action in its place. The noop action is accepted as an argument in the step(action, no_op_action) function. nan_zeros_v0 ( env ) ¶ If an action is a NaN value for a step, the following wrapper will trigger a warning and perform a zeros action in its place. nan_random_v0 ( env ) ¶ If an action is a NaN value for a step, the following wrapper will trigger a warning and perform a random action in its place. The random action will be retrieved from the action mask. scale_actions_v0 ( env , scale ) ¶ Scales the high and low bounds of the action space by the scale argument in __init__(). Additionally, scales any actions by the same value when step() is called. Included Multi-Agent Only Functions ¶ agent_indicator_v0 ( env , type_only = False ) ¶ Adds an indicator of the agent ID to the observation, only supports discrete and 1D, 2D, and 3D box. For 1d spaces, the agent ID is converted to a 1-hot vector and appended to the observation (increasing the size of the observation space as necessary). 2d and 3d spaces are treated as images (with channels last) and the ID is converted to n additional channels with the channel that represents the ID as all 1s and the other channel as all 0s (a sort of one hot encoding). This allows MADRL methods like parameter sharing to learn policies for heterogeneous agents since the policy can tell what agent it’s acting on. Set the type_only parameter to parse the name of the agent as <type>_<n> and have the appended 1-hot vector only identify the type, rather than the specific agent name. This is useful for games where there are many agents in an environment but few types of agents. Agent indication for MADRL was first introduced in Cooperative Multi-Agent Control Using Deep Reinforcement Learning. black_death_v2 ( env ) ¶ Instead of removing dead actions, observations and rewards are 0 and actions are ignored. This can simplify handling agent death mechanics. The name “black death” does not come from the plague, but from the fact that you see a black image (an image filled with zeros) when you die. pad_action_space_v0 ( env ) ¶ Pads the action spaces of all agents to be be the same as the biggest, per the algorithm posed in Parameter Sharing is Surprisingly Useful for Deep Reinforcement Learning . This enables MARL methods that require homogeneous action spaces for all agents to work with environments with heterogeneous action spaces. Discrete actions inside the padded region will be set to zero, and Box actions will be cropped down to the original space. pad_observations_v0 ( env ) ¶ Pads observations to be of the shape of the largest observation of any agent with 0s, per the algorithm posed in Parameter Sharing is Surprisingly Useful for Deep Reinforcement Learning . This enables MARL methods that require homogeneous observations from all agents to work in environments with heterogeneous observations. This currently supports Discrete and Box observation spaces. Citation ¶ If you use this in your research, please cite: @article { SuperSuit , Title = { SuperSuit : Simple Microwrappers for Reinforcement Learning Environments }, Author = { Terry , J K and Black , Benjamin and Hari , Ananth }, journal = { arXiv preprint arXiv : 2008.08932 }, year = { 2020 } }

